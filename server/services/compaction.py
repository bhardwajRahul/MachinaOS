"""Memory compaction service using native provider APIs.

Anthropic: tool_runner with compaction_control parameter
OpenAI: context_management with compact_threshold
Others: Client-side summarization fallback

Threshold strategy: per-session custom_threshold > model-aware threshold > global default.
Model-aware threshold = 50% of model's context window (e.g., 100K for a 200K model).
"""

from pydantic import BaseModel
from datetime import datetime, timezone
from typing import Dict, Any, Optional, TYPE_CHECKING

from core.logging import get_logger
from services.pricing import get_pricing_service

if TYPE_CHECKING:
    from core.database import Database
    from core.config import Settings

logger = get_logger(__name__)

# Fraction of context window used as compaction threshold when no custom threshold is set
CONTEXT_THRESHOLD_RATIO = 0.5


def _extract_text_from_response(content) -> str:
    """Extract text content from various LLM response formats.

    Handles:
    - String content (OpenAI, Anthropic)
    - List of content blocks (Gemini format: [{"type": "text", "text": "..."}])
    - Other complex formats
    """
    # Handle list content (Gemini format)
    if isinstance(content, list):
        text_parts = []
        for block in content:
            if isinstance(block, dict):
                if block.get('type') == 'text' and block.get('text'):
                    text_parts.append(block['text'])
                elif 'text' in block:
                    text_parts.append(str(block['text']))
            elif isinstance(block, str):
                text_parts.append(block)
        return '\n'.join(text_parts)

    # Handle string content
    if isinstance(content, str):
        return content

    # Fallback
    return str(content) if content else ""


class CompactionConfig(BaseModel):
    """Provider-agnostic compaction configuration."""
    enabled: bool = True
    threshold: int = 100000


class CompactionService:
    """Minimal compaction service using native provider APIs."""

    def __init__(self, database: "Database", settings: "Settings"):
        self._db = database
        self._config = CompactionConfig(
            enabled=settings.compaction_enabled,
            threshold=settings.compaction_threshold
        )
        self._ai_service = None

    def set_ai_service(self, ai_service) -> None:
        """Wire AI service for generating compaction summaries."""
        self._ai_service = ai_service

    def get_model_threshold(self, model: str, provider: str) -> int:
        """Compute compaction threshold based on model's context window.

        Returns 50% of the model's context length, floored to the global
        default if the registry has no data.  Per-session custom_threshold
        (checked in track()) still takes priority over this value.
        """
        from services.model_registry import get_model_registry
        registry = get_model_registry()
        context_length = registry.get_context_length(model, provider)
        model_threshold = int(context_length * CONTEXT_THRESHOLD_RATIO)
        # Never go below the configured minimum (ge=10000 in Settings)
        return max(model_threshold, 10000)

    def anthropic_config(self, threshold: Optional[int] = None, model: str = "", provider: str = "anthropic") -> Dict[str, Any]:
        """Anthropic SDK compaction_control for tool_runner."""
        effective = threshold or self.get_model_threshold(model, provider)
        return {
            "enabled": self._config.enabled,
            "context_token_threshold": effective
        }

    def anthropic_api_config(self, threshold: Optional[int] = None, model: str = "", provider: str = "anthropic") -> Dict[str, Any]:
        """Anthropic Messages API context_management config."""
        effective = threshold or self.get_model_threshold(model, provider)
        return {
            "betas": ["compact-2026-01-12"],
            "context_management": {
                "edits": [{
                    "type": "compact_20260112",
                    "trigger": {"type": "input_tokens", "value": effective}
                }]
            }
        }

    def openai_config(self, threshold: Optional[int] = None, model: str = "", provider: str = "openai") -> Dict[str, Any]:
        """OpenAI context_management config."""
        effective = threshold or self.get_model_threshold(model, provider)
        return {"context_management": {"compact_threshold": effective}}

    async def track(self, session_id: str, node_id: str, provider: str, model: str, usage: Dict[str, int]) -> Dict[str, Any]:
        """Track token usage and cost, return compaction status."""
        # Calculate cost using PricingService
        pricing_service = get_pricing_service()
        cost = pricing_service.calculate_cost(
            provider=provider,
            model=model,
            input_tokens=usage.get("input_tokens", 0),
            output_tokens=usage.get("output_tokens", 0),
            cache_read_tokens=usage.get("cache_read_tokens", 0),
            cache_creation_tokens=usage.get("cache_creation_tokens", 0),
            reasoning_tokens=usage.get("reasoning_tokens", 0)
        )

        # Save token metric with cost fields
        await self._db.save_token_metric({
            "session_id": session_id,
            "node_id": node_id,
            "provider": provider,
            "model": model,
            **usage,
            "input_cost": cost["input_cost"],
            "output_cost": cost["output_cost"],
            "cache_cost": cost["cache_cost"],
            "total_cost": cost["total_cost"]
        })

        state = await self._db.get_or_create_session_token_state(session_id)
        new_total = state["cumulative_total"] + usage.get("total_tokens", 0)
        new_total_cost = state.get("cumulative_total_cost", 0.0) + cost["total_cost"]

        # Update cumulative state with cost
        await self._db.update_session_token_state(session_id, {
            "cumulative_input_tokens": state["cumulative_input_tokens"] + usage.get("input_tokens", 0),
            "cumulative_output_tokens": state["cumulative_output_tokens"] + usage.get("output_tokens", 0),
            "cumulative_total": new_total,
            "cumulative_input_cost": state.get("cumulative_input_cost", 0.0) + cost["input_cost"],
            "cumulative_output_cost": state.get("cumulative_output_cost", 0.0) + cost["output_cost"],
            "cumulative_total_cost": new_total_cost
        })

        # Priority: per-session custom > model-aware > global default
        custom = state.get("custom_threshold")
        if custom:
            threshold = custom
        else:
            threshold = self.get_model_threshold(model, provider)

        return {
            "total": new_total,
            "total_cost": new_total_cost,
            "cost": cost,
            "threshold": threshold,
            "needs_compaction": self._config.enabled and new_total >= threshold
        }

    async def record(self, session_id: str, node_id: str, provider: str, model: str, tokens_before: int, tokens_after: int, summary: Optional[str] = None) -> None:
        """Record compaction event after native API handles it."""
        state = await self._db.get_or_create_session_token_state(session_id)
        await self._db.save_compaction_event({
            "session_id": session_id, "node_id": node_id, "trigger_reason": "native",
            "tokens_before": tokens_before, "tokens_after": tokens_after,
            "summary_model": model, "summary_provider": provider, "success": True, "summary_content": summary
        })
        await self._db.update_session_token_state(session_id, {
            "cumulative_total": tokens_after,
            "last_compaction_at": datetime.now(timezone.utc),
            "compaction_count": state["compaction_count"] + 1
        })

    async def stats(self, session_id: str, model: str = "", provider: str = "") -> Dict[str, Any]:
        """Get session statistics.

        When model/provider are provided, the threshold reflects the model's
        context window.  Otherwise falls back to the global default.
        """
        state = await self._db.get_or_create_session_token_state(session_id)
        custom = state.get("custom_threshold")
        if custom:
            threshold = custom
        elif model and provider:
            threshold = self.get_model_threshold(model, provider)
        else:
            threshold = self._config.threshold
        return {
            "session_id": session_id,
            "total": state["cumulative_total"],
            "threshold": threshold,
            "count": state.get("compaction_count", 0),
        }

    async def configure(self, session_id: str, threshold: Optional[int] = None, enabled: Optional[bool] = None) -> bool:
        """Configure session settings."""
        updates = {}
        if threshold is not None:
            updates["custom_threshold"] = threshold
        if enabled is not None:
            updates["compaction_enabled"] = enabled
        return await self._db.update_session_token_state(session_id, updates) if updates else True

    async def compact_context(
        self,
        session_id: str,
        node_id: str,
        memory_content: str,
        provider: str,
        api_key: str,
        model: str,
    ) -> Dict[str, Any]:
        """Perform compaction by summarizing conversation history.

        Uses the AI service to generate a structured summary following Claude Code pattern.
        """
        if not self._ai_service:
            logger.warning("[Compaction] AI service not wired")
            return {"success": False, "error": "AI service not available"}

        if not memory_content or len(memory_content.strip()) < 100:
            return {"success": False, "error": "Memory content too short"}

        state = await self._db.get_or_create_session_token_state(session_id)
        tokens_before = state["cumulative_total"]

        try:
            prompt = f"""Summarize this conversation into a structured format:

## Task Overview
What the user is trying to accomplish.

## Current State
What's been completed and what's in progress.

## Important Discoveries
Key findings, decisions, or problems encountered.

## Next Steps
What needs to happen next.

## Context to Preserve
Details that must be retained for continuity.

---
CONVERSATION:
{memory_content}
---

Provide a concise but complete summary."""

            # Use a reasonable summary size: min(4096, model's max output)
            from services.model_registry import get_model_registry
            model_max = get_model_registry().get_max_output_tokens(model, provider)
            summary_tokens = min(4096, model_max)

            llm = self._ai_service.create_model(
                provider=provider, api_key=api_key, model=model,
                temperature=0.3, max_tokens=summary_tokens
            )

            from langchain_core.messages import HumanMessage
            response = await llm.ainvoke([HumanMessage(content=prompt)])
            raw_content = response.content if hasattr(response, 'content') else str(response)
            summary = _extract_text_from_response(raw_content)

            new_memory = f"# Conversation Summary (Compacted)\n*Generated: {datetime.now(timezone.utc).isoformat()}*\n\n{summary}"

            await self.record(session_id, node_id, provider, model, tokens_before, 0, new_memory)
            logger.info(f"[Compaction] Session {session_id}: {tokens_before} -> 0 tokens")

            return {"success": True, "summary": new_memory, "tokens_before": tokens_before, "tokens_after": 0}

        except Exception as e:
            logger.error(f"[Compaction] Failed: {e}")
            return {"success": False, "error": str(e)}


_service: Optional[CompactionService] = None

def get_compaction_service() -> Optional[CompactionService]:
    return _service

def init_compaction_service(database: "Database", settings: "Settings") -> CompactionService:
    global _service
    _service = CompactionService(database, settings)
    logger.info("[Compaction] Initialized")
    return _service
